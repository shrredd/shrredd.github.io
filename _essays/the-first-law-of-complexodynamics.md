---
title: "The First Law of Complexodynamics"
subtitle: "Why interesting systems rise, peak, and fade"
byline: "Shravan Reddy &middot; February 2026"
description: "Takeaways from Scott Aaronson's The First Law of Complexodynamics: why interestingness can rise and fall while entropy still increases."
twitter_description: "Why interestingness rises and falls even when entropy only goes up."
date_published: "2026-02-08"
---
Ilya Sutskever recommended [30 papers](https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE) with a tantalizing promise: "If you really learn all of these, you'll know 90% of what matters today." This essay is my takeaway from Scott Aaronson's [*The First Law of Complexodynamics*](https://scottaaronson.blog/?p=762).
{: .lede}

The central question is simple but deep: if entropy tends to increase monotonically, why does a system's *interestingness* often rise, peak, and then decline?

Milk poured into espresso is a good intuition pump. At the start, separate layers are neat but dull. Then comes the marbled swirl phase, which feels rich and structured. Eventually, everything blends into a uniform mixture and becomes boring again.

The same pattern appears elsewhere. An ordered crystal is simple. A fully random state is also simple in a different sense: there is no coherent structure to explain. The middle phase often carries the most meaningful structure.

## Kolmogorov Complexity as Compression

Aaronson starts with Kolmogorov complexity (KC): the length of the shortest computer program that can generate a string.

- A highly ordered string can be generated by a tiny loop, so KC is low.
- A truly random string has no shorter description than itself, so KC is high.

One useful way to internalize KC is as a measure of lossless compressibility. If the shortest possible program is tiny, the object is highly compressible. If the shortest program is large, compression buys you very little.

## Why Randomness Is Not Sophistication

KC alone does not match our intuitive notion of sophistication.

A random 1000-bit string can have very high KC, but it still does not feel meaningful. Its origin story can be trivial: coin flips or some other random process.

So high KC is not enough to define *interestingness*.

## Sophistication (Complextropy)

To close that gap, Aaronson introduces sophistication, also called complextropy.

The idea is to split description length into two parts:

1. A model that captures regularities.
2. Extra bits to specify one concrete instance under that model.

Pure order needs almost no model. Pure noise needs almost no model either, because the model is just "randomness." The richest zone is in between, where structure exists but is non-trivial.

That middle regime matches our intuition for interesting systems: snowflakes, marbled coffee, and language-like artifacts.

## Why This Matters for Neural Networks

My practical takeaway is that training a neural network is, in large part, a compression problem.

Model weights are a compact encoding of patterns in data. If the model is too small, it cannot encode enough structure and will underfit. If it is too large and unconstrained, it can memorize noise and lose generalization.

Compression is insight. Randomness is not inherently interesting. Peak complexity is often transient.
{: .closing}
